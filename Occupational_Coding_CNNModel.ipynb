{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model for Occupational Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Read data and output\n",
    "\n",
    "import jieba # Word segmentation\n",
    "import jieba.analyse as analyse\n",
    "\n",
    "import numpy as np # Data processing\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, MaxPool1D, Conv1D, Convolution1D\n",
    "from keras.layers import Embedding\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import load_model\n",
    "\n",
    "from gensim.models import KeyedVectors # Import pre-trained word vectos\n",
    "\n",
    "import matplotlib.pyplot as plt # Draw graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "Set up for training the model<br>\n",
    "Variables needed to be set: word2vec_path, stopwords_path, vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file paths needed and dimension of the pre-trained word vectors\n",
    "\n",
    "stopwords_path = 'stopword.txt' # Stop-words file\n",
    "word2vec_path = 'tencent-ailab-embedding-zh-d200-v0.2.0-s.txt' # Pre-trained word vectors file\n",
    "\n",
    "vec_size = 200 # Deimension of the pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-trained word vectors\n",
    "\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(word2vec_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare functions used\n",
    "\n",
    "# Create list of stop-words\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    " \n",
    " \n",
    "# Word segmentation\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.lcut(sentence.strip())\n",
    "    stopwords = stopwordslist(stopwords_path)\n",
    "    outstr = []\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords: # Delete the stop-words\n",
    "            if word != '\\t' and word != ' ': # Delete spaces\n",
    "                outstr.append(word)\n",
    "    return outstr\n",
    "\n",
    "# Change the words segemented into word vectors\n",
    "def transform_to_matrix(x, padding_size=10, vec_size=vec_size):\n",
    "    res = []\n",
    "    for sen in x:\n",
    "        matrix = []\n",
    "        for i in range(padding_size):\n",
    "            try:\n",
    "                matrix.append(wv_from_text[sen[i]].tolist())\n",
    "            except:\n",
    "                matrix.append([0] * vec_size)\n",
    "        res.append(matrix)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model; add convolution layers, pooling layer, etc.\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256,\n",
    "                3,\n",
    "                padding = 'same',\n",
    "                activation = 'relu'))\n",
    "model.add(MaxPool1D(3,3,padding='same'))\n",
    "model.add(Conv1D(32,3,padding='same', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 100000, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "             optimizer = \"adam\",\n",
    "             metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "Variables needed to be set: training_data_path, train_sample, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "\n",
    "training_data_path = 'hy_training.xlsx' # Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data and tags\n",
    "train_pd = pd.read_excel(training_data_path, index_col=0)\n",
    "\n",
    "train_sample = 'QG302' # Column name of data\n",
    "train_label = 'QG302CODE' # Column name of tags\n",
    "\n",
    "# Get data and tags and stored as Numpy arrays\n",
    "x_train = train_pd[train_sample] # training data\n",
    "x_train = np.array(x_train)\n",
    "y_train = train_pd[train_label] # tags\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call seg_sentence function to do segmentation\n",
    "temp = []\n",
    "for i in x_train:\n",
    "    line_seg = seg_sentence(i)\n",
    "    temp.append(line_seg)\n",
    "\n",
    "x_train = temp\n",
    "\n",
    "# Convert the variable x_train tp matrix\n",
    "x_train = transform_to_matrix(x_train)\n",
    "\n",
    "# Store x_train and y_train as Numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Variables needed to be set: batch_size, epochs, val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "batch_size = 128 # Batch size\n",
    "epochs = 30 # Number of epochs\n",
    "val_split = 0.1 # Test data percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Output shows training loss(loss), training accuracy(accuracy), validation loss(val_loss), and validation accuracy(val_acc)\n",
    "history = model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            validation_split = val_split,\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw graph of loss and accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('hangye model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(0,31,2))\n",
    "plt.yticks(np.arange(0,1,0.05))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('hangye model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "Variables needed to be set: save_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model_path = 'model_hy.h5' # Path of the model to be saved\n",
    "model.save(save_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7994908c881ac51252aa6b158559f16fb3043709f3a2bdd1c7e20729cc39315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
